\documentclass[12pt,letterpaper]{hmcpset}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{parskip}
\usepackage{bm}

% info for header block in upper right hand corner
\name{Evan Hubinger and Jonah Rubin}
\class{CS 152}
\assignment{Paper Report}

\newcommand{\m}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\vecof}[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand{\tx}[1]{\text{#1}}
\newcommand{\pn}[1]{\left(#1\right)}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\bk}[1]{\left[#1\right]}
\newcommand{\abk}[1]{\left\langle#1\right\rangle}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\ellipsis}{\,\ldots}
\newcommand{\given}{\,|\,}
\newcommand{\where}{\mid}
\newcommand{\bbm}[1]{\mathbb{#1}}
\newcommand{\impl}{\rightarrow}
\newcommand{\dubimpl}{\leftrightarrow}

\begin{document}
{Playing Atari with Deep Reinforcement Learning}\\
This paper proposes a new method of reinforcement learning for Atari games called Deep Q-Learning. Q-Learning is...
\\
The paper's method uses several new ideas, including replay memory which enables the network to randomly access previous actions. The method was also notable for being able to learn seven seperate games with the same network architecture and hyperparamaters. The network outperformed the state of the art algorithms on six of the seven games, and outperformed humans on three of the seven. The technology has been used to create later, more advanced Deep Q-Learning networks which play better.
\end{document}
